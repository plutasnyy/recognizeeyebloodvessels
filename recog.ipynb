{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "recog.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/plutasnyy/recognizeeyebloodvessels/blob/master/recog.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "7O3ySi1WGGMl",
        "colab_type": "code",
        "outputId": "022b0ec3-4359-478b-b670-4ec7d62106e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "cell_type": "code",
      "source": [
        "# You can change the directory name\n",
        "LOG_DIR = 'tb_logs'\n",
        "\n",
        "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "!unzip ngrok-stable-linux-amd64.zip\n",
        "\n",
        "import os\n",
        "if not os.path.exists(LOG_DIR):\n",
        "  os.makedirs(LOG_DIR)\n",
        "  \n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR))\n",
        "\n",
        "get_ipython().system_raw('./ngrok http 6006 &')\n",
        "\n",
        "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-04-28 09:36:10--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.207.111.186, 52.203.53.176, 52.7.169.168, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.207.111.186|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 14991793 (14M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "\r          ngrok-sta   0%[                    ]       0  --.-KB/s               \r         ngrok-stab  10%[=>                  ]   1.52M  7.49MB/s               \rngrok-stable-linux- 100%[===================>]  14.30M  40.5MB/s    in 0.4s    \n",
            "\n",
            "2019-04-28 09:36:11 (40.5 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [14991793/14991793]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n",
            "https://c927d19f.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2DhbcDQM9LLn",
        "colab_type": "code",
        "outputId": "99013a26-eb3a-4514-995d-b01ba01fbdd0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        }
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import keras\n",
        "from keras import backend as K\n",
        "from keras import Sequential\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense, BatchNormalization, Activation\n",
        "from keras.utils import to_categorical\n",
        "from keras.optimizers import SGD\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "from collections import Counter\n",
        "from time import gmtime, strftime\n",
        "\n",
        "from PIL import Image\n",
        "from numpy import asarray\n",
        "from skimage import transform, exposure\n",
        "from skimage.filters import sobel\n",
        "\n",
        "import sklearn\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "PATCH_SIZE = 48\n",
        "HALF_OF_PATCH_SIZE = int(PATCH_SIZE / 2)\n",
        "SPLIT_PATCHES_SIZE = 80000\n",
        "\n",
        "\n",
        "def correct_image(image):\n",
        "    #TODO run this function only when process tensor, not during creating an object\n",
        "    logging.info('Correct an image with shape: {}'.format(image.shape))\n",
        "    bw_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
        "    image_adapt = exposure.equalize_adapthist(bw_image)\n",
        "    logarithmic_corrected = exposure.adjust_log(image_adapt, 1)\n",
        "    return logarithmic_corrected\n",
        "  \n",
        "class Tensor:\n",
        "    def __init__(self, base_image, vessels, mask, id):\n",
        "        assert base_image.size == vessels.size, 'Images have different sizes'\n",
        "        assert mask.size == vessels.size, 'Mask has wrong size'\n",
        "\n",
        "        long_edge = max(base_image.size)\n",
        "        scale = LONG_EDGE_SIZE / long_edge\n",
        "        scale=1\n",
        "        w, h = base_image.size\n",
        "        c = len(base_image.getbands())\n",
        "        w, h = int(w * scale), int(h * scale)\n",
        "        logging.info(\n",
        "            'Tensor resize from {} to {} with {} scale'.format((base_image.size, c), (w, h, c), round(scale, 2)))\n",
        "\n",
        "        self.base_image = asarray(base_image.resize((w, h), resample=Image.NEAREST))  # 0-255\n",
        "        self.corrected = correct_image(self.base_image)  # 0-1 TODO VERY BAD DEPENDENCY it should be moved out from this class\n",
        "        self.vessels = (asarray(vessels.resize((w, h), resample=Image.NEAREST)) / 255).astype(int)  # 0-1\n",
        "\n",
        "        if len(self.vessels.shape) == 3:\n",
        "            self.vessels = self.vessels[:, :, 1]\n",
        "\n",
        "        self.mask = (asarray(mask.resize((w, h))) / 255).astype(int)  # 0-1\n",
        "        self.id = id\n",
        "\n",
        "    def draw_tensor(self):\n",
        "        \"\"\"\n",
        "        TODO This is very bad dependency to other functionality, this class should be independent, it was created only for tests and it should be removed when will be unused\n",
        "        \"\"\"\n",
        "        from image_processor import draw_images\n",
        "        draw_images([self.base_image, self.corrected, self.vessels, self.mask])\n",
        "\n",
        "    def __repr__(self):\n",
        "        return '{}: base_image: {}'.format(self.id, self.base_image.shape)\n",
        "\n",
        "def create_tensor_from_file():\n",
        "    images_path = sorted(glob.glob('/content/recognizeeyebloodvessels/train/*'))\n",
        "    print(images_path)\n",
        "    for i in range(0, len(images_path), 3):\n",
        "        file_name = images_path[i].replace('/', '.').split('.')[1]\n",
        "        logging.info('Process filepath: {}, fileName: {}, i: {}'.format(images_path[i], file_name, i))\n",
        "        base_image = Image.open(images_path[i])\n",
        "        mask = Image.open(images_path[i + 1])\n",
        "        vessels = Image.open(images_path[i + 2])\n",
        "        yield Tensor(base_image, vessels, mask, i)\n",
        "\n",
        "def create_samples_from_tensor(tensor: Tensor):\n",
        "    logging.info('Create samples from tensor: {}'.format(tensor))\n",
        "    X, Y = list(), list()\n",
        "    for (x, y), value in np.ndenumerate(tensor.mask):\n",
        "        if x + PATCH_SIZE <= tensor.corrected.shape[0] and y + PATCH_SIZE <= tensor.corrected.shape[1]:\n",
        "            center_x, center_y = x + HALF_OF_PATCH_SIZE, y + HALF_OF_PATCH_SIZE\n",
        "            if tensor.mask[center_x][center_y] == 1:\n",
        "                X.append(tensor.corrected[x: x + PATCH_SIZE, y: y + PATCH_SIZE])\n",
        "                Y.append(tensor.vessels[center_x][center_y])\n",
        "    return X, Y\n",
        "\n",
        "\n",
        "def random_undersampling(X, y):\n",
        "    \"\"\"\n",
        "    In this moment we will lose order of samples\n",
        "    \"\"\"\n",
        "    minority_value, majority_value = 1, 0\n",
        "    new_X, new_y = list(), list()\n",
        "    length = len(y)\n",
        "    quantity_of_minority = sum(y)\n",
        "    quantity_of_majority = length - quantity_of_minority\n",
        "    indexes_list = list(range(length))\n",
        "    random.shuffle(indexes_list)\n",
        "    skipped, to_skip = 0, quantity_of_majority - quantity_of_minority\n",
        "    assert to_skip >= 0\n",
        "    for index in indexes_list:\n",
        "        if skipped < to_skip and y[index] == majority_value:\n",
        "            skipped += 1\n",
        "        else:\n",
        "            new_X.append(X[index])\n",
        "            new_y.append(y[index])\n",
        "\n",
        "    result_X, result_Y = sklearn.utils.shuffle(new_X, new_y, random_state=0)\n",
        "    return result_X, result_Y\n",
        "\n",
        "\n",
        "def draw_images(images: list):\n",
        "    logging.info('Draw {} images'.format(len(images)))\n",
        "    size = np.ceil(np.sqrt(len(images)))\n",
        "    fig = plt.figure(figsize=(32, 32))\n",
        "    for i, img in enumerate(images):\n",
        "        fig.add_subplot(size, size, i + 1)\n",
        "        plt.imshow(img)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def draw_grey_image(image):\n",
        "    plt.imshow(image, cmap='gray')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def preprocess_image(tensor: Tensor):\n",
        "    logging.info('Started preprocess a tensor: {}'.format(tensor))\n",
        "    edge_sobel = sobel(tensor.corrected)\n",
        "    normalized_image = cv2.normalize(edge_sobel, None, alpha=0, beta=255, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
        "    _, thresholded_image = cv2.threshold(normalized_image, 20, 255, cv2.THRESH_BINARY)\n",
        "    cleaned_image = (thresholded_image * tensor.mask).astype(int)\n",
        "    return np.invert(cleaned_image)\n",
        "\n",
        "class Model:\n",
        "    def __init__(self):\n",
        "        self.model = self._build_model()\n",
        "        filepath = 'new_best2.hdf5'\n",
        "        self.tb_call_back = keras.callbacks.TensorBoard(log_dir=LOG_DIR, histogram_freq=0, write_graph=True,\n",
        "                                                        write_images=True)\n",
        "        self.checkpoint = ModelCheckpoint(filepath, save_weights_only=False, monitor='val_acc', verbose=0,\n",
        "                                          save_best_only=True, mode='max')\n",
        "\n",
        "    def load_weights(self, filename):\n",
        "        logging.info('Load weights: {}'.format(filename))\n",
        "        self.model.load_weights(filename)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        self.model.fit(X, y, validation_split=0.1, epochs=150, batch_size=32,\n",
        "                       callbacks=[self.tb_call_back, self.checkpoint], verbose=1)\n",
        "\n",
        "    def compile(self):\n",
        "        sgd = keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=False)\n",
        "        adam = keras.optimizers.Adam(lr=1e-5, epsilon=None, decay=0.0, amsgrad=False)\n",
        "        self.model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "        logging.info('Compiled keras model')\n",
        "\n",
        "    def evaluate(self, X, y):\n",
        "        result = self.model.evaluate(X, y)\n",
        "        logging.info('Model evaluated, loss: {} acc: {}'.format(result[0], result[1]))\n",
        "        return result\n",
        "\n",
        "    def predict(self, X):\n",
        "        input = X\n",
        "        if len(input.shape) == 2:\n",
        "            input = input.reshape(1, PATCH_SIZE, PATCH_SIZE, 1)\n",
        "            logging.debug('Reshaped before prediction')\n",
        "        result = self.model.predict(input)\n",
        "        logging.debug('Predicted: {}, return {}'.format(result, np.argmax(result)))\n",
        "        logging.debug('For: {}'.format(input))\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    def _build_model():\n",
        "        model = Sequential()\n",
        "        model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                         activation='relu',\n",
        "                         input_shape=(PATCH_SIZE, PATCH_SIZE, 1)))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Conv2D(32, (3, 3), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "\n",
        "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation(\"relu\"))\n",
        "        model.add(Conv2D(128, (3, 3), activation='relu'))\n",
        "\n",
        "        model.add(Flatten())\n",
        "        model.add(Dense(256, activation='relu'))\n",
        "        model.add(Dropout(0.2))\n",
        "        model.add(Dense(128, activation='relu'))\n",
        "        model.add(Dense(2, activation='softmax'))\n",
        "#         model = Sequential()\n",
        "#         model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "#                          activation='relu',\n",
        "#                          input_shape=(PATCH_SIZE, PATCH_SIZE, 1)))\n",
        "#         model.add(Conv2D(64, (3, 3), activation='relu'))\n",
        "#         model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#         model.add(Dropout(0.25))\n",
        "#         model.add(Flatten())\n",
        "#         model.add(Dense(128, activation='relu'))\n",
        "#         model.add(Dropout(0.5))\n",
        "#         model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "        logging.info('Created keras model')\n",
        "        return model\n",
        "      \n",
        "model = Model()\n",
        "# model.load_weights('best.hdf5')\n",
        "model.compile()\n",
        "\n",
        "for tensor in create_tensor_from_file():\n",
        "    X, y = create_samples_from_tensor(tensor)\n",
        "\n",
        "    logging.info('Patches were created')\n",
        "    print(len(X),len(y))\n",
        "    logging.info('Original dataset shape {}'.format(Counter(y)))\n",
        "    X, y = random_undersampling(X, y)\n",
        "\n",
        "    logging.debug('Resampled dataset shape {}'.format(Counter(y)))\n",
        "\n",
        "    for start_index in range(0, len(X), SPLIT_PATCHES_SIZE):\n",
        "        end_index = min(start_index + SPLIT_PATCHES_SIZE, len(X))  # tricky way to avoid OutOfIndexError\n",
        "        logging.info('Splitting set. Range: {}:{} Progress of this tensor: {}% Time: {}'.format(\n",
        "            start_index, end_index, round(start_index / len(X) * 100), strftime(\"%Y-%m-%d %H:%M:%S\", gmtime())))\n",
        "        X_subset, y_subset = X[start_index:end_index], y[start_index:end_index]\n",
        "        logging.debug('Cut dataset result countered shape {}'.format(Counter(y_subset)))\n",
        "\n",
        "        X_subset = np.array(X_subset).reshape(len(X_subset), PATCH_SIZE, PATCH_SIZE, 1)\n",
        "        y_subset = to_categorical(y_subset)\n",
        "        logging.debug('Shape X: {}, y: {}'.format(X_subset.shape, y_subset.shape))\n",
        "\n",
        "        model.fit(X_subset, y_subset)\n",
        "        break\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "INFO:root:Created keras model\n",
            "INFO:root:Compiled keras model\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "z6Knq5GMTex4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf tb_logs* Graph"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w6U0RMtkKLGe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dj4Djgz6F_f7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    }
  ]
}